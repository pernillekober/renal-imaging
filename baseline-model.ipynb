{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b642b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nilearn.image import concat_imgs, mean_img, resample_img\n",
    "\n",
    "import tensorflow as tflow\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from keras.layers.core import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "\n",
    "import json \n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from random import sample\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2595455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_00000.nii.gz case_00075.nii.gz case_00150.nii.gz case_00225.nii.gz\r\n",
      "case_00001.nii.gz case_00076.nii.gz case_00151.nii.gz case_00226.nii.gz\r\n",
      "case_00002.nii.gz case_00077.nii.gz case_00152.nii.gz case_00227.nii.gz\r\n",
      "case_00003.nii.gz case_00078.nii.gz case_00153.nii.gz case_00228.nii.gz\r\n",
      "case_00004.nii.gz case_00079.nii.gz case_00154.nii.gz case_00229.nii.gz\r\n",
      "case_00005.nii.gz case_00080.nii.gz case_00155.nii.gz case_00230.nii.gz\r\n",
      "case_00006.nii.gz case_00081.nii.gz case_00156.nii.gz case_00231.nii.gz\r\n",
      "case_00007.nii.gz case_00082.nii.gz case_00157.nii.gz case_00232.nii.gz\r\n",
      "case_00008.nii.gz case_00083.nii.gz case_00158.nii.gz case_00233.nii.gz\r\n",
      "case_00009.nii.gz case_00084.nii.gz case_00159.nii.gz case_00234.nii.gz\r\n",
      "case_00010.nii.gz case_00085.nii.gz case_00160.nii.gz case_00235.nii.gz\r\n",
      "case_00011.nii.gz case_00086.nii.gz case_00161.nii.gz case_00236.nii.gz\r\n",
      "case_00012.nii.gz case_00087.nii.gz case_00162.nii.gz case_00237.nii.gz\r\n",
      "case_00013.nii.gz case_00088.nii.gz case_00163.nii.gz case_00238.nii.gz\r\n",
      "case_00014.nii.gz case_00089.nii.gz case_00164.nii.gz case_00239.nii.gz\r\n",
      "case_00015.nii.gz case_00090.nii.gz case_00165.nii.gz case_00240.nii.gz\r\n",
      "case_00016.nii.gz case_00091.nii.gz case_00166.nii.gz case_00241.nii.gz\r\n",
      "case_00017.nii.gz case_00092.nii.gz case_00167.nii.gz case_00242.nii.gz\r\n",
      "case_00018.nii.gz case_00093.nii.gz case_00168.nii.gz case_00243.nii.gz\r\n",
      "case_00019.nii.gz case_00094.nii.gz case_00169.nii.gz case_00244.nii.gz\r\n",
      "case_00020.nii.gz case_00095.nii.gz case_00170.nii.gz case_00245.nii.gz\r\n",
      "case_00021.nii.gz case_00096.nii.gz case_00171.nii.gz case_00246.nii.gz\r\n",
      "case_00022.nii.gz case_00097.nii.gz case_00172.nii.gz case_00247.nii.gz\r\n",
      "case_00023.nii.gz case_00098.nii.gz case_00173.nii.gz case_00248.nii.gz\r\n",
      "case_00024.nii.gz case_00099.nii.gz case_00174.nii.gz case_00249.nii.gz\r\n",
      "case_00025.nii.gz case_00100.nii.gz case_00175.nii.gz case_00250.nii.gz\r\n",
      "case_00026.nii.gz case_00101.nii.gz case_00176.nii.gz case_00251.nii.gz\r\n",
      "case_00027.nii.gz case_00102.nii.gz case_00177.nii.gz case_00252.nii.gz\r\n",
      "case_00028.nii.gz case_00103.nii.gz case_00178.nii.gz case_00253.nii.gz\r\n",
      "case_00029.nii.gz case_00104.nii.gz case_00179.nii.gz case_00254.nii.gz\r\n",
      "case_00030.nii.gz case_00105.nii.gz case_00180.nii.gz case_00255.nii.gz\r\n",
      "case_00031.nii.gz case_00106.nii.gz case_00181.nii.gz case_00256.nii.gz\r\n",
      "case_00032.nii.gz case_00107.nii.gz case_00182.nii.gz case_00257.nii.gz\r\n",
      "case_00033.nii.gz case_00108.nii.gz case_00183.nii.gz case_00258.nii.gz\r\n",
      "case_00034.nii.gz case_00109.nii.gz case_00184.nii.gz case_00259.nii.gz\r\n",
      "case_00035.nii.gz case_00110.nii.gz case_00185.nii.gz case_00260.nii.gz\r\n",
      "case_00036.nii.gz case_00111.nii.gz case_00186.nii.gz case_00261.nii.gz\r\n",
      "case_00037.nii.gz case_00112.nii.gz case_00187.nii.gz case_00262.nii.gz\r\n",
      "case_00038.nii.gz case_00113.nii.gz case_00188.nii.gz case_00263.nii.gz\r\n",
      "case_00039.nii.gz case_00114.nii.gz case_00189.nii.gz case_00264.nii.gz\r\n",
      "case_00040.nii.gz case_00115.nii.gz case_00190.nii.gz case_00265.nii.gz\r\n",
      "case_00041.nii.gz case_00116.nii.gz case_00191.nii.gz case_00266.nii.gz\r\n",
      "case_00042.nii.gz case_00117.nii.gz case_00192.nii.gz case_00267.nii.gz\r\n",
      "case_00043.nii.gz case_00118.nii.gz case_00193.nii.gz case_00268.nii.gz\r\n",
      "case_00044.nii.gz case_00119.nii.gz case_00194.nii.gz case_00269.nii.gz\r\n",
      "case_00045.nii.gz case_00120.nii.gz case_00195.nii.gz case_00270.nii.gz\r\n",
      "case_00046.nii.gz case_00121.nii.gz case_00196.nii.gz case_00271.nii.gz\r\n",
      "case_00047.nii.gz case_00122.nii.gz case_00197.nii.gz case_00272.nii.gz\r\n",
      "case_00048.nii.gz case_00123.nii.gz case_00198.nii.gz case_00273.nii.gz\r\n",
      "case_00049.nii.gz case_00124.nii.gz case_00199.nii.gz case_00274.nii.gz\r\n",
      "case_00050.nii.gz case_00125.nii.gz case_00200.nii.gz case_00275.nii.gz\r\n",
      "case_00051.nii.gz case_00126.nii.gz case_00201.nii.gz case_00276.nii.gz\r\n",
      "case_00052.nii.gz case_00127.nii.gz case_00202.nii.gz case_00277.nii.gz\r\n",
      "case_00053.nii.gz case_00128.nii.gz case_00203.nii.gz case_00278.nii.gz\r\n",
      "case_00054.nii.gz case_00129.nii.gz case_00204.nii.gz case_00279.nii.gz\r\n",
      "case_00055.nii.gz case_00130.nii.gz case_00205.nii.gz case_00280.nii.gz\r\n",
      "case_00056.nii.gz case_00131.nii.gz case_00206.nii.gz case_00281.nii.gz\r\n",
      "case_00057.nii.gz case_00132.nii.gz case_00207.nii.gz case_00282.nii.gz\r\n",
      "case_00058.nii.gz case_00133.nii.gz case_00208.nii.gz case_00283.nii.gz\r\n",
      "case_00059.nii.gz case_00134.nii.gz case_00209.nii.gz case_00284.nii.gz\r\n",
      "case_00060.nii.gz case_00135.nii.gz case_00210.nii.gz case_00285.nii.gz\r\n",
      "case_00061.nii.gz case_00136.nii.gz case_00211.nii.gz case_00286.nii.gz\r\n",
      "case_00062.nii.gz case_00137.nii.gz case_00212.nii.gz case_00287.nii.gz\r\n",
      "case_00063.nii.gz case_00138.nii.gz case_00213.nii.gz case_00288.nii.gz\r\n",
      "case_00064.nii.gz case_00139.nii.gz case_00214.nii.gz case_00289.nii.gz\r\n",
      "case_00065.nii.gz case_00140.nii.gz case_00215.nii.gz case_00290.nii.gz\r\n",
      "case_00066.nii.gz case_00141.nii.gz case_00216.nii.gz case_00291.nii.gz\r\n",
      "case_00067.nii.gz case_00142.nii.gz case_00217.nii.gz case_00292.nii.gz\r\n",
      "case_00068.nii.gz case_00143.nii.gz case_00218.nii.gz case_00293.nii.gz\r\n",
      "case_00069.nii.gz case_00144.nii.gz case_00219.nii.gz case_00294.nii.gz\r\n",
      "case_00070.nii.gz case_00145.nii.gz case_00220.nii.gz case_00295.nii.gz\r\n",
      "case_00071.nii.gz case_00146.nii.gz case_00221.nii.gz case_00296.nii.gz\r\n",
      "case_00072.nii.gz case_00147.nii.gz case_00222.nii.gz case_00297.nii.gz\r\n",
      "case_00073.nii.gz case_00148.nii.gz case_00223.nii.gz case_00298.nii.gz\r\n",
      "case_00074.nii.gz case_00149.nii.gz case_00224.nii.gz case_00299.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls './preprocessed-data/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678b2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rootdir = './preprocessed-data/images'\n",
    "# image_paths_list = []\n",
    "# annotation_paths_list = []\n",
    "\n",
    "# for file in os.listdir(rootdir):\n",
    "#     d = os.path.join(rootdir, file)\n",
    "#     image_paths_list.append(d)\n",
    "    \n",
    "    \n",
    "# image_dict = {}\n",
    "\n",
    "# for img_path in image_paths_list:\n",
    "#     case_id = img_path[27:37]\n",
    "#     ct_nii = nib.load(img_path).get_fdata()\n",
    "#     print(case_id, ct_nii.shape)\n",
    "#     image_dict[case_id] = ct_nii\n",
    "\n",
    "# with open('../kits21/kits21/data/kits.json') as user_file:\n",
    "#     file_contents = user_file.read()\n",
    "\n",
    "# meta_list = json.loads(file_contents)\n",
    "\n",
    "# labels_dict = {}\n",
    "# for case in meta_list:\n",
    "#     c_id = case['case_id']\n",
    "#     labels_dict[c_id] = case['malignant']\n",
    "\n",
    "# for key, value in labels_dict.items():\n",
    "#     print(key, value)\n",
    "    \n",
    "    \n",
    "# img_list, id_list, label_list = [], [], []\n",
    "\n",
    "# common_keys = labels_dict.keys() & image_case_dict.keys()\n",
    "\n",
    "# for k in common_keys:\n",
    "#     id_list.append(k)\n",
    "#     img_list.append(image_case_dict[k])\n",
    "#     label_list.append(labels_dict[k])\n",
    "    \n",
    "# print(len(img_list), len(id_list), len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08c3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def malignant_labels_to_dict(json_file_path):\n",
    "    \"\"\"Takes path to json file and stores malignant label for each \n",
    "    patient in a dictionary\"\"\"\n",
    "    \n",
    "    with open(json_file_path) as user_file:\n",
    "          file_contents = user_file.read()\n",
    "\n",
    "    meta_list = json.loads(file_contents)\n",
    "\n",
    "    labels_dict = {}\n",
    "    for case in meta_list:\n",
    "        c_id = case['case_id']\n",
    "        labels_dict[c_id] = case['malignant']\n",
    "    \n",
    "    return labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d60cd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsample_id_list(n_samples, pct_of_total_neg, dict_of_labels):\n",
    "    subsample_id_list = []\n",
    "    total_neg = len(dict_of_labels) - sum(dict_of_labels.values())\n",
    "    n_neg = math.ceil(total_neg*(pct_of_total_neg/100))\n",
    "    n_pos = n_samples - n_neg\n",
    "    \n",
    "    true_list = [k for k,v in dict_of_labels.items() if v == True]\n",
    "    false_list = [k for k,v in dict_of_labels.items() if v == False]\n",
    "    \n",
    "    subsample_id_list.extend(false_list[:n_neg])\n",
    "    subsample_id_list.extend(sample(true_list,n_pos))\n",
    "                             \n",
    "    print('Generated list of {0} case IDs of {1} positive and {2} negative labels'.format(\n",
    "        len(subsample_id_list), n_neg, n_pos))\n",
    "    \n",
    "    return subsample_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb77a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_image_paths(rootdir, subfolder):\n",
    "    \"\"\" Creates list of paths to images in each subfolder and corresponsing patient ID \"\"\"\n",
    "    \n",
    "    folder_path = rootdir + subfolder\n",
    "    paths_list = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        paths_list.append(folder_path + '/' + file)\n",
    "    \n",
    "    #for file in os.listdir(rootdir_str):\n",
    "    #    d = os.path.join(rootdir_str, file)\n",
    "    #    if os.path.isdir(d):\n",
    "    #        paths_list.append(d + general_filename)\n",
    "            \n",
    "    return paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1610674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti_img_and_mask_as_numpy(paths_list, subsample_list):\n",
    "    \"\"\" Loads nifti images corresponding to paths lists in a dictionary \n",
    "    of case_id as keys and 3D numpy image arrays as values. Filters list by  \"\"\"\n",
    "\n",
    "    image_dict = {}\n",
    "\n",
    "    for img_path in paths_list:\n",
    "        case_id = img_path[-17:-7]\n",
    "        if case_id in subsample_list:\n",
    "            print('match on: ', case_id)\n",
    "            ct_nii = nib.load(img_path).get_fdata()\n",
    "            image_dict[case_id] = ct_nii\n",
    "    \n",
    "    return image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83bbfd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in labels_dict:  300\n",
      "Generated list of 100 case IDs of 25 positive and 75 negative labels\n",
      "Is image and mask path list the same lenght?:  True\n",
      "match on:  case_00201\n",
      "match on:  case_00246\n",
      "match on:  case_00258\n",
      "match on:  case_00225\n",
      "match on:  case_00134\n",
      "match on:  case_00034\n",
      "match on:  case_00173\n",
      "match on:  case_00010\n",
      "match on:  case_00161\n",
      "match on:  case_00102\n",
      "match on:  case_00002\n",
      "match on:  case_00147\n",
      "match on:  case_00155\n",
      "match on:  case_00055\n",
      "match on:  case_00163\n",
      "match on:  case_00063\n",
      "match on:  case_00112\n",
      "match on:  case_00071\n",
      "match on:  case_00211\n",
      "match on:  case_00235\n",
      "match on:  case_00082\n",
      "match on:  case_00190\n",
      "match on:  case_00244\n",
      "match on:  case_00116\n",
      "match on:  case_00104\n",
      "match on:  case_00167\n",
      "match on:  case_00032\n",
      "match on:  case_00051\n",
      "match on:  case_00020\n",
      "match on:  case_00120\n",
      "match on:  case_00289\n",
      "match on:  case_00043\n",
      "match on:  case_00252\n",
      "match on:  case_00215\n",
      "match on:  case_00276\n",
      "match on:  case_00184\n",
      "match on:  case_00088\n",
      "match on:  case_00188\n",
      "match on:  case_00242\n",
      "match on:  case_00096\n",
      "match on:  case_00221\n",
      "match on:  case_00266\n",
      "match on:  case_00205\n",
      "match on:  case_00278\n",
      "match on:  case_00118\n",
      "match on:  case_00106\n",
      "match on:  case_00077\n",
      "match on:  case_00114\n",
      "match on:  case_00069\n",
      "match on:  case_00169\n",
      "match on:  case_00299\n",
      "match on:  case_00130\n",
      "match on:  case_00287\n",
      "match on:  case_00072\n",
      "match on:  case_00135\n",
      "match on:  case_00156\n",
      "match on:  case_00193\n",
      "match on:  case_00093\n",
      "match on:  case_00212\n",
      "match on:  case_00234\n",
      "match on:  case_00249\n",
      "match on:  case_00238\n",
      "match on:  case_00202\n",
      "match on:  case_00070\n",
      "match on:  case_00113\n",
      "match on:  case_00125\n",
      "match on:  case_00058\n",
      "match on:  case_00029\n",
      "match on:  case_00154\n",
      "match on:  case_00137\n",
      "match on:  case_00269\n",
      "match on:  case_00241\n",
      "match on:  case_00253\n",
      "match on:  case_00187\n",
      "match on:  case_00050\n",
      "match on:  case_00284\n",
      "match on:  case_00133\n",
      "match on:  case_00042\n",
      "match on:  case_00288\n",
      "match on:  case_00121\n",
      "match on:  case_00117\n",
      "match on:  case_00017\n",
      "match on:  case_00078\n",
      "match on:  case_00294\n",
      "match on:  case_00286\n",
      "match on:  case_00031\n",
      "match on:  case_00052\n",
      "match on:  case_00107\n",
      "match on:  case_00019\n",
      "match on:  case_00119\n",
      "match on:  case_00176\n",
      "match on:  case_00216\n",
      "match on:  case_00275\n",
      "match on:  case_00279\n",
      "match on:  case_00232\n",
      "match on:  case_00085\n",
      "match on:  case_00251\n",
      "match on:  case_00197\n",
      "match on:  case_00097\n",
      "match on:  case_00189\n",
      "match on:  case_00201\n",
      "match on:  case_00246\n",
      "match on:  case_00258\n",
      "match on:  case_00225\n",
      "match on:  case_00134\n",
      "match on:  case_00034\n",
      "match on:  case_00173\n",
      "match on:  case_00010\n",
      "match on:  case_00161\n",
      "match on:  case_00102\n",
      "match on:  case_00002\n",
      "match on:  case_00147\n",
      "match on:  case_00155\n",
      "match on:  case_00055\n",
      "match on:  case_00163\n",
      "match on:  case_00063\n",
      "match on:  case_00112\n",
      "match on:  case_00071\n",
      "match on:  case_00211\n",
      "match on:  case_00235\n",
      "match on:  case_00082\n",
      "match on:  case_00190\n",
      "match on:  case_00244\n",
      "match on:  case_00116\n",
      "match on:  case_00104\n",
      "match on:  case_00167\n",
      "match on:  case_00032\n",
      "match on:  case_00051\n",
      "match on:  case_00020\n",
      "match on:  case_00120\n",
      "match on:  case_00289\n",
      "match on:  case_00043\n",
      "match on:  case_00252\n",
      "match on:  case_00215\n",
      "match on:  case_00276\n",
      "match on:  case_00184\n",
      "match on:  case_00088\n",
      "match on:  case_00188\n",
      "match on:  case_00242\n",
      "match on:  case_00096\n",
      "match on:  case_00221\n",
      "match on:  case_00266\n",
      "match on:  case_00205\n",
      "match on:  case_00278\n",
      "match on:  case_00118\n",
      "match on:  case_00106\n",
      "match on:  case_00077\n",
      "match on:  case_00114\n",
      "match on:  case_00069\n",
      "match on:  case_00169\n",
      "match on:  case_00299\n",
      "match on:  case_00130\n",
      "match on:  case_00287\n",
      "match on:  case_00072\n",
      "match on:  case_00135\n",
      "match on:  case_00156\n",
      "match on:  case_00193\n",
      "match on:  case_00093\n",
      "match on:  case_00212\n",
      "match on:  case_00234\n",
      "match on:  case_00249\n",
      "match on:  case_00238\n",
      "match on:  case_00202\n",
      "match on:  case_00070\n",
      "match on:  case_00113\n",
      "match on:  case_00125\n",
      "match on:  case_00058\n",
      "match on:  case_00029\n",
      "match on:  case_00154\n",
      "match on:  case_00137\n",
      "match on:  case_00269\n",
      "match on:  case_00241\n",
      "match on:  case_00253\n",
      "match on:  case_00187\n",
      "match on:  case_00050\n",
      "match on:  case_00284\n",
      "match on:  case_00133\n",
      "match on:  case_00042\n",
      "match on:  case_00288\n",
      "match on:  case_00121\n",
      "match on:  case_00117\n",
      "match on:  case_00017\n",
      "match on:  case_00078\n",
      "match on:  case_00294\n",
      "match on:  case_00286\n",
      "match on:  case_00031\n",
      "match on:  case_00052\n",
      "match on:  case_00107\n",
      "match on:  case_00019\n",
      "match on:  case_00119\n",
      "match on:  case_00176\n",
      "match on:  case_00216\n",
      "match on:  case_00275\n",
      "match on:  case_00279\n",
      "match on:  case_00232\n",
      "match on:  case_00085\n",
      "match on:  case_00251\n",
      "match on:  case_00197\n",
      "match on:  case_00097\n",
      "match on:  case_00189\n",
      "Is image and mask dictionaries the same lenght?: True\n"
     ]
    }
   ],
   "source": [
    "#------------- Generate dictionary of case_id and binary malignant label --------\n",
    "labels_dict = malignant_labels_to_dict('../kits21/kits21/data/kits.json')\n",
    "print('Number of labels in labels_dict: ', len(labels_dict))\n",
    "\n",
    "#------------- Sample n instances from labels_dict with all possible negative cases --------\n",
    "subsample_list = generate_subsample_id_list(100, 100, labels_dict)\n",
    "\n",
    "#------------- Generate list of paths to preprocessed images --------\n",
    "rootdir = './preprocessed-data/'\n",
    "\n",
    "image_paths_list = processed_image_paths(rootdir, 'images')\n",
    "mask_paths_list = processed_image_paths(rootdir, 'masks')\n",
    "\n",
    "print('Is image and mask path list the same lenght?: ', len(image_paths_list) == len(mask_paths_list))\n",
    "\n",
    "#------------- Load images and masks filtered by subsample --------\n",
    "image_dict = load_nifti_img_and_mask_as_numpy(image_paths_list, subsample_list)\n",
    "mask_dict = load_nifti_img_and_mask_as_numpy(mask_paths_list, subsample_list)\n",
    "\n",
    "print('Is image and mask dictionaries the same lenght?:', len(image_dict) == len(mask_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857b05f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are cases and labels lists the same length?:  True\n"
     ]
    }
   ],
   "source": [
    "cases = subsample_list\n",
    "\n",
    "labels =  [labels_dict[x] for x in cases]\n",
    "\n",
    "print(\"Are cases and labels lists the same length?: \", len(cases) == len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "754ceaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train, id_test_temp, labels_train, labels_test_temp = train_test_split(\n",
    "    cases, labels, test_size=0.30, shuffle=True, stratify=labels)\n",
    "\n",
    "id_val, id_test, labels_val, labels_test = train_test_split(\n",
    "    id_test_temp, labels_test_temp, test_size=0.50, shuffle=True, random_state=42, stratify=labels_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcdde4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3) [0.]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def fill_set(x_set, y_set, image_dictionary, label_dictionary):\n",
    "    x_list, y_list = [], []\n",
    "    \n",
    "    for idx, (x, y) in enumerate(zip(x_set, y_set)):    \n",
    "        for img in image_dictionary[x]:\n",
    "            x_list.append(img)\n",
    "            y_list.append(label_dictionary[x])\n",
    "    \n",
    "    x_array, y_array = np.array(x_list), np.array(y_list).astype(int)\n",
    "    #x_array = np.array(tf.expand_dims(x_array, -1))\n",
    "    x_array = np.repeat(x_array[..., np.newaxis], 3, -1)\n",
    "\n",
    "    y_array = np.asarray(y_array).astype('float32').reshape((-1,1))\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "x_train, y_train = fill_set(id_train, labels_train, image_dict, labels_dict)\n",
    "x_val, y_val = fill_set(id_val, labels_val, image_dict, labels_dict)\n",
    "#x_test, y_test = fill_set(id_test, labels_test, images_dict_preprocessed, labels_dict)\n",
    "\n",
    "\n",
    "print(x_train[0].shape, y_train[0])\n",
    "print(len(x_train) == len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "845b10a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15980, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b438df",
   "metadata": {},
   "source": [
    "## upsampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06ee0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# x_train_reshape = x_train.reshape(x_train.shape[0], 224 * 224 * 3)\n",
    "# sm = SMOTE(random_state=42)\n",
    "# x_smote, y_smote = sm.fit_resample(x_train_reshape, y_train)\n",
    "# x_smote = x_smote.reshape(x_smote.shape[0], 224, 224, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443c8af",
   "metadata": {},
   "source": [
    "## Developing baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c654135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 100352)            0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               25690368  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,319,297\n",
      "Trainable params: 40,707,585\n",
      "Non-trainable params: 8,611,712\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 18:49:41.138620: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - ETA: 0s - loss: 0.4922 - auc: 0.5701 - accuracy: 0.8201"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 18:52:28.280616: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 189s 2s/step - loss: 0.4922 - auc: 0.5701 - accuracy: 0.8201 - val_loss: 0.7089 - val_auc: 0.6174 - val_accuracy: 0.6192\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 184s 2s/step - loss: 0.4560 - auc: 0.5857 - accuracy: 0.8300 - val_loss: 0.7335 - val_auc: 0.6122 - val_accuracy: 0.6192\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 184s 2s/step - loss: 0.4525 - auc: 0.5842 - accuracy: 0.8298 - val_loss: 1.0469 - val_auc: 0.6098 - val_accuracy: 0.7653\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 185s 2s/step - loss: 0.4375 - auc: 0.5904 - accuracy: 0.8310 - val_loss: 2.3214 - val_auc: 0.6646 - val_accuracy: 0.7429\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 185s 2s/step - loss: 0.4535 - auc: 0.5849 - accuracy: 0.8213 - val_loss: 4.6672 - val_auc: 0.7144 - val_accuracy: 0.7844\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 184s 2s/step - loss: 0.4380 - auc: 0.5957 - accuracy: 0.8305 - val_loss: 8.1015 - val_auc: 0.6639 - val_accuracy: 0.7084\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 185s 2s/step - loss: 0.4364 - auc: 0.5920 - accuracy: 0.8311 - val_loss: 8.5572 - val_auc: 0.6837 - val_accuracy: 0.7289\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 183s 2s/step - loss: 0.4365 - auc: 0.5975 - accuracy: 0.8308 - val_loss: 9.5910 - val_auc: 0.7103 - val_accuracy: 0.7699\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 185s 2s/step - loss: 0.4366 - auc: 0.5867 - accuracy: 0.8311 - val_loss: 14.1216 - val_auc: 0.6032 - val_accuracy: 0.6846\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 186s 2s/step - loss: 0.4351 - auc: 0.6023 - accuracy: 0.8311 - val_loss: 12.7525 - val_auc: 0.6541 - val_accuracy: 0.7219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xe91d60cd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draft model\n",
    "resnet = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3), classes=1)\n",
    "\n",
    "for layer in resnet.layers[:143]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics_list = [tf.keras.metrics.AUC(name = 'auc'),\n",
    "                tf.keras.metrics.BinaryAccuracy(name = 'accuracy')]\n",
    "\n",
    "\n",
    "\n",
    "#calculate class weights\n",
    "class_weights = {0 : 30, 1 : 1}\n",
    "\n",
    "optimizer_fn = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.00002, jit_compile = False)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(resnet)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = optimizer_fn, loss='binary_crossentropy', metrics= metrics_list)\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, validation_data = (x_val, y_val), class_weight = class_weights, epochs=10, batch_size = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77b24aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 23s 322ms/step - loss: 12.7525 - auc: 0.6541 - accuracy: 0.7219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12.752480506896973, 0.6540908217430115, 0.7218852043151855]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_val, y_val)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9cf5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.687402069568634, 0.7713485956192017] auc / accuracy if the model 143 layers are not trainable / 10 poch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd2f8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change metrics tresholds and etc\n",
    "\n",
    "# add more layers\n",
    "\n",
    "# run models on random 100 images \n",
    "\n",
    "#tf.keras.metrics.SpecificityAtSensitivity(0.1, name = 'specificity'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6da64787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://openreview.net/pdf?id=XXtHQy0d8Y paper on segmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
