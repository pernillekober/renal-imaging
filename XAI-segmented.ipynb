{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e6be8a",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For importing modules\n",
    "import os, sys\n",
    "\n",
    "#For file handling\n",
    "from glob import glob\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json \n",
    "\n",
    "#For image processing\n",
    "import PIL\n",
    "import cv2\n",
    "import nibabel as nib\n",
    "import nibabel.processing \n",
    "from nibabel.processing import resample_to_output\n",
    "import nilearn\n",
    "\n",
    "\n",
    "#Image visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Other\n",
    "import numpy as np\n",
    "import math\n",
    "from random import sample\n",
    "\n",
    "#Modeling\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nilearn.image import concat_imgs, mean_img, resample_img\n",
    "\n",
    "import tensorflow as tflow\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56718ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nilearn.image import concat_imgs, mean_img, resample_img\n",
    "\n",
    "import tensorflow as tflow\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, BatchNormalization\n",
    "from keras.layers.core import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "\n",
    "import json \n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from random import sample\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib as mpl\n",
    "from ipywidgets import interact, widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "#import warnings\n",
    "from ipywidgets import IntSlider\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import lime\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb9e6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/path/to/2014_07_13_test')\n",
    "import generate_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e94cb",
   "metadata": {},
   "source": [
    "## Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "697c3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation set\n",
    "val_case = ['case_00233', 'case_00089', 'case_00050', 'case_00112', 'case_00258', 'case_00246', 'case_00157', 'case_00149','case_00184']\n",
    "test_case = ['case_00221', 'case_00259', 'case_00087', 'case_00254', 'case_00098', 'case_00023', 'case_00041',\n",
    "             'case_00080', 'case_00101', 'case_00164', 'case_00002', 'case_00110', 'case_00030', 'case_00068',\n",
    "             'case_00026', 'case_00063', 'case_00006', 'case_00048', 'case_00250', 'case_00238', \n",
    "             'case_00111', 'case_00278', 'case_00133', 'case_00284', 'case_00282', 'case_00269', 'case_00039',\n",
    "             'case_00033', 'case_00108', 'case_00175', 'case_00161', 'case_00256', 'case_00119', 'case_00286',\n",
    "             'case_00077', 'case_00162', 'case_00270', 'case_00271', 'case_00285', 'case_00174', 'case_00147',\n",
    "             'case_00215', 'case_00150', 'case_00052', 'case_00231', 'case_00198', 'case_00117', 'case_00138',\n",
    "             'case_00211', 'case_00190', 'case_00248', 'case_00235', 'case_00049', 'case_00074', 'case_00107',\n",
    "             'case_00218', 'case_00001', 'case_00193', 'case_00067', 'case_00072', 'case_00044',\n",
    "             'case_00294', 'case_00298', 'case_00263', 'case_00038', 'case_00299', 'case_00249', 'case_00225',\n",
    "             'case_00217', 'case_00178', 'case_00082', 'case_00035', 'case_00034', 'case_00047', 'case_00276',\n",
    "             'case_00151', 'case_00226', 'case_00086', 'case_00176']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b78348d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_part_label_dict(labels_dict, list_of_cases):\n",
    "    \"\"\" Generate lists of pathes to the right files \"\"\"\n",
    "    return {k: labels_dict[k] for k in list_of_cases}\n",
    "\n",
    "def load_data(rootdir, img_dir, json_path, list_cases):\n",
    "    \"\"\" Load data in the right format \"\"\"\n",
    "    val_path = generate_input.processed_image_paths(rootdir, img_dir)\n",
    "    labels_dict = generate_input.malignant_labels_to_dict(json_path)\n",
    "    labels_dict_short = generate_part_label_dict(labels_dict, list_cases)\n",
    "    image_dict_short = generate_input.load_nifti_img_and_mask_as_numpy(val_path, list_cases)\n",
    "    x_set, y_set = generate_input.fill_set(list_cases, labels_dict_short, image_dict_short, labels_dict)\n",
    "    x_set = generate_input.adding_channel(x_set)\n",
    "    return x_set, y_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c1139b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_channel_dict(image_dict):\n",
    "    \"\"\" Adds channels to each 3D image in the dictionary\"\"\"\n",
    "    for key in image_dict:\n",
    "        image_dict[key] = generate_input.adding_channel(image_dict[key])\n",
    "    return image_dict\n",
    "\n",
    "\n",
    "def empty_image_removal_dict(masks, images):\n",
    "    \"\"\" Removes padding of 0 from images and masks \"\"\"\n",
    "    x_final_dict = {}\n",
    "    \n",
    "    common_keys = masks.keys() & images.keys()\n",
    "    for key in common_keys:\n",
    "        x_final = []\n",
    "\n",
    "        for i, pair in enumerate(zip(masks[key], images[key])):\n",
    "    \n",
    "            if len(np.unique(pair[0])) > 1:\n",
    "                x_final.append(pair[1])\n",
    "            \n",
    "        x_final = np.array(x_final)\n",
    "\n",
    "        \n",
    "        x_final_dict[key] = x_final\n",
    "    return x_final_dict\n",
    "\n",
    "def load_data_to_evaluate_by_majority_voting(rootdir, img_dir, json_path, list_cases):\n",
    "    \"\"\" Loads data in the right format \"\"\"\n",
    "    val_path = generate_input.processed_image_paths(rootdir, img_dir)\n",
    "    labels_dict = generate_input.malignant_labels_to_dict(json_path)\n",
    "    labels_dict_short = generate_part_label_dict(labels_dict, list_cases)\n",
    "    labels_dict_short = {key: float(value) for key, value in labels_dict_short.items()}\n",
    "    \n",
    "    image_dict_short = generate_input.load_nifti_img_and_mask_as_numpy(val_path, list_cases)\n",
    "    image_dict_short = adding_channel_dict(image_dict_short)\n",
    "    return image_dict_short, labels_dict_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de325bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict, test_label_dict = load_data_to_evaluate_by_majority_voting('./cropped-data/', 'images-64', '../kits21/kits21/data/kits.json', test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "308146c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_masks, test_label = load_data_to_evaluate_by_majority_voting('./cropped-data/', 'masks-64', '../kits21/kits21/data/kits.json', test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ec2aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_short = empty_image_removal_dict(test_dict_masks, test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46a81b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model64croped_segmented3.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model64croped_segmented3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4971df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "optimizer_fn = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.001, jit_compile = False)\n",
    "metrics_list = [tf.keras.metrics.AUC(name = 'auc'),\n",
    "                tf.keras.metrics.BinaryAccuracy(name = 'accuracy')]\n",
    "loaded_model.compile(optimizer = optimizer_fn, loss='binary_crossentropy', metrics= metrics_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f42d23",
   "metadata": {},
   "source": [
    "## Application of the XAI techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51a4a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://usmanr149.github.io/urmlblog/cnn/2020/05/01/Salincy-Maps.html\n",
    "def get_saliency_map(image, model):\n",
    "    image =tf.Variable(image, dtype = float)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        predictions = model(image) \n",
    "        loss = predictions[:, 0]\n",
    "    \n",
    "    gradient = tape.gradient(loss, image)\n",
    "    gradient = tf.math.abs(gradient)\n",
    "    gradient = np.max(gradient, axis=3)[0]\n",
    "    \n",
    "    # normalize between 0 and 1\n",
    "    min_val, max_val = np.min(gradient), np.max(gradient)\n",
    "    smap = (gradient - min_val) / (max_val - min_val + tf.keras.backend.epsilon())\n",
    "    return smap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5aa1194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://keras.io/examples/vision/grad_cam/\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    \n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, pred_index]\n",
    "\n",
    "    gradient = tape.gradient(loss, last_conv_layer_output)\n",
    "\n",
    "    pooled_grads = tf.reduce_mean(gradient, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    \n",
    "    return heatmap.numpy()\n",
    "\n",
    "def display_gradcam(image, heatmap, alpha=0.4):\n",
    "    \n",
    "    image = np.squeeze(image)\n",
    "    \n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = mpl.colormaps.get_cmap('jet')\n",
    "    \n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((image.shape[1], image.shape[0]))\n",
    "    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    \n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + image\n",
    "    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "    \n",
    "    return superimposed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22ed0480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_dict(image_dict, model):\n",
    "    \"\"\" Create a dictionary with patient level predictions \"\"\"\n",
    "    pred_dict = {}\n",
    "    for key in image_dict:\n",
    "        y_pred = model.predict(image_dict[key])\n",
    "        voting = np.round(np.sum(y_pred)/len(y_pred))\n",
    "        if voting > 0.5:\n",
    "            pred_dict[key] = 'Malignant'\n",
    "        else:\n",
    "            pred_dict[key] = 'Benign'\n",
    "        \n",
    "    return pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fc5d51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 23:45:52.035287: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 999ms/step\n",
      "1/1 [==============================] - 1s 549ms/step\n",
      "4/4 [==============================] - 1s 40ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 1s 509ms/step\n",
      "1/1 [==============================] - 1s 560ms/step\n",
      "2/2 [==============================] - 1s 602ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 1s 665ms/step\n",
      "2/2 [==============================] - 1s 679ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 1s 701ms/step\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 1s 343ms/step\n",
      "2/2 [==============================] - 1s 685ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 1s 672ms/step\n",
      "2/2 [==============================] - 1s 810ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 1s 668ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "2/2 [==============================] - 1s 674ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "2/2 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 1s 667ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 1s 878ms/step\n",
      "1/1 [==============================] - 1s 774ms/step\n",
      "2/2 [==============================] - 1s 698ms/step\n",
      "2/2 [==============================] - 1s 742ms/step\n",
      "2/2 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 1s 701ms/step\n",
      "1/1 [==============================] - 0s 378ms/step\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 1s 250ms/step\n",
      "4/4 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 1s 668ms/step\n",
      "2/2 [==============================] - 0s 335ms/step\n",
      "2/2 [==============================] - 1s 697ms/step\n",
      "2/2 [==============================] - 1s 671ms/step\n",
      "4/4 [==============================] - 0s 45ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "2/2 [==============================] - 0s 371ms/step\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 1s 673ms/step\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 404ms/step\n",
      "1/1 [==============================] - 0s 363ms/step\n",
      "1/1 [==============================] - 0s 395ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "2/2 [==============================] - 0s 426ms/step\n",
      "2/2 [==============================] - 1s 716ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "4/4 [==============================] - 0s 67ms/step\n"
     ]
    }
   ],
   "source": [
    "majority_prediction = create_prediction_dict(test_dict_short, loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebccd23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_slice_prediction(slices, model):\n",
    "    \"\"\" Creates slice prediction and converts it to textual explanation \"\"\"\n",
    "    slices = slices.reshape((1,) + slices.shape) \n",
    "    slices_pred = float(model.predict(slices, verbose=None))\n",
    "    if slices_pred > 0.5:\n",
    "        return 'Malignant', round(((slices_pred-0.5)/(1-0.5))*100, 2) \n",
    "\n",
    "    else:\n",
    "        return 'Benign', round((((1-slices_pred) -0.5)/(1-0.5))*100, 2)  \n",
    "    \n",
    "def rename_label(label):\n",
    "    if label == 1.0:\n",
    "        return 'Malignant'\n",
    "    else:\n",
    "        return 'Benign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ac3dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %matplotlib inline\n",
    "model = loaded_model\n",
    "last_conv_layer_name = \"conv4_block6_3_conv\"\n",
    "dataset = test_dict_short\n",
    "labels = test_label_dict\n",
    "patient_widget = dataset.keys()\n",
    "slices = IntSlider(min=0, max=50, value = 20)\n",
    "prediction = majority_prediction\n",
    "malignant_widget = widgets.RadioButtons(options=['Malignant class', 'Benign class'], description='Contribution to:', disabled=False)\n",
    "\n",
    "def f(Patient, Slice_number, Malignant):\n",
    "    \n",
    "    try:\n",
    "        # update how long the second widget\n",
    "        slices.max =len(dataset[Patient]) \n",
    "    \n",
    "        slice_prediction = create_slice_prediction(dataset[Patient][Slice_number], model)\n",
    "        \n",
    "        def predicted(img_array):\n",
    "        # create a prediction function for the LIME model\n",
    "            return model.predict(img_array, verbose=False)\n",
    "        explainer = lime_image.LimeImageExplainer(random_state=42)\n",
    "        explanation = explainer.explain_instance(dataset[Patient][Slice_number], predicted)\n",
    "        \n",
    "        print(\"Case # \" + Patient + ' Label: ' + str(rename_label(labels[Patient]) ))\n",
    "        print(\"Case was predicted as \" + str(prediction[Patient]))\n",
    "        print('This slice was predicited as ' + str(slice_prediction[0]) + ' with ' +  str(slice_prediction[1]) + ' % probability')\n",
    "        \n",
    "        \n",
    "        # take image\n",
    "        img_array = dataset[Patient][Slice_number:Slice_number+1]\n",
    "        \n",
    "        # create subplots\n",
    "        fig = plt.figure(figsize = (11,6))\n",
    "        grid = ImageGrid(fig, 111, nrows_ncols = (2,3), axes_pad = 0.4, cbar_location = \"right\",\n",
    "                         cbar_mode=\"single\", cbar_size=\"2%\", cbar_pad=0.05)\n",
    "        \n",
    "\n",
    "\n",
    "        # create raw image\n",
    "        ax1 = grid[0].imshow(dataset[Patient][Slice_number])\n",
    "        \n",
    "        #create guided backpropagation\n",
    "        smap = get_saliency_map(img_array, model)\n",
    "        ax2 = grid[1].imshow(smap, cmap=\"RdBu_r\")\n",
    "        \n",
    "        # create grad-cam\n",
    "        heatmap_cam = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    "        grad_cam = display_gradcam(img_array, heatmap_cam, alpha=0.002)\n",
    "        ax3 = grid[2].imshow(grad_cam, cmap=\"RdBu_r\")\n",
    "        plt.colorbar(ax3, cax=grid.cbar_axes[0])\n",
    "        \n",
    "        # create LIME\n",
    "        \n",
    "        heatmap_LIME = np.vectorize(dict(explanation.local_exp[0]).get)(explanation.segments)\n",
    "\n",
    "        if Malignant == 'Malignant class':\n",
    "            image, mask = explanation.get_image_and_mask(0, positive_only = True, negative_only = False, hide_rest=True,  num_features=1)\n",
    "            ax5 = grid[4].imshow(heatmap_LIME, cmap=\"RdBu_r\")\n",
    "        else:\n",
    "            image, mask = explanation.get_image_and_mask(0, positive_only = False, negative_only = True, hide_rest=True,  num_features=1)\n",
    "            ax5 = grid[4].imshow(heatmap_LIME, cmap=\"RdBu\")\n",
    "        ax4 = grid[3].imshow(mark_boundaries(image, mask))  \n",
    "\n",
    "    \n",
    "        grid[0].title.set_text('Raw image')\n",
    "        grid[1].title.set_text('Guided backpropagation')\n",
    "        grid[2].title.set_text('Grad-CAM')\n",
    "        #LIME part\n",
    "        grid[3].title.set_text('Parts contrbuted the prediction')\n",
    "        grid[4].title.set_text('Heatmap of all parts')\n",
    "    \n",
    "        grid[0].axis('off')\n",
    "        grid[1].axis('off')\n",
    "        grid[2].axis('off')\n",
    "        grid[3].axis('off')\n",
    "        grid[4].axis('off')\n",
    "        grid[5].axis('off')\n",
    "        \n",
    "    except IndexError:\n",
    "        # update how long the second widget\n",
    "        slices.max =len(dataset[Patient])\n",
    "        slices.value = len(dataset[Patient])\n",
    "        print('Change slice number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb011f93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c65c2b8df4c46daa357dc133c841d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Patient', options=('case_00107', 'case_00077', 'case_00299', 'case…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(Patient, Slice_number, Malignant)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(f, Patient = patient_widget, Slice_number = slices, Malignant = malignant_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42a2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
